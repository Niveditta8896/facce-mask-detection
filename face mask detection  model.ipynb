{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e630e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3b4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"C:/hope/deep learning/face mask detection/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903fd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[\"with_mask\",\"without_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34718639",
   "metadata": {},
   "source": [
    "# now we r going to join the data path to the foler wher each category is present for os to easily access the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c441afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for category in categories: \n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    label=categories.index(category)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        img=cv2.imread(file_path)\n",
    "        if img is not None:\n",
    "            image=cv2.resize(img,(224,224))# detection models prefer this size\n",
    "            dataset.append([image,label])\n",
    "        \n",
    "    # this whole function is for one image from for till dataset .append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2802f64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 259, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09578d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)# so we hv in total 3823 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6733f4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape# shape of the first image in dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb3678",
   "metadata": {},
   "source": [
    "# 2. we r going to split the dataset to x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415d783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x == numpy array of images with pixel values, y=labels\n",
    "x=[]\n",
    "y=[]\n",
    "for image ,label in dataset:\n",
    "    x.append(image)\n",
    "    y.append(label)\n",
    "    \n",
    "    # once this is done we get x as an list of array of pixel values and y as a lisst both we need to convert before model creation\n",
    "    #in to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237a3d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3823, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5965a885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3823,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e927b2",
   "metadata": {},
   "source": [
    "# 3. now we r going to normalize x and y to standard values btw 0 and 1 before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece7428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x/255 # where the 255 is the maximum pixel intensity value.\n",
    "# for y we do one hot encoding after spliting y? to first clearly distinguish y_train,y_test then encode if required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e1474",
   "metadata": {},
   "source": [
    "# 4. now we r going to split x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6593b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tt\n",
    "x_train,x_test,y_train,y_test=tt(x,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6569ed5",
   "metadata": {},
   "source": [
    "# now lets encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e803b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical # theres no np_utils in the new version of keras\n",
    "y_train=to_categorical(y_train,num_classes=2)\n",
    "y_test=to_categorical(y_test,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5787f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2676, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b17ef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1147, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2496c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing check list \n",
    "\n",
    "\n",
    "\n",
    "# Loaded images\n",
    "# Resized to same shape\n",
    "# Converted to NumPy arrays\n",
    "# Normalized X (0â€“1\n",
    "# Split into train/test\n",
    "# One-hot encoded y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32095b",
   "metadata": {},
   "source": [
    "# 5. model creation using transfer learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7240fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i am using mobilenetv2 for faster tring,it hs already trained on multiple images so can easily recognise features on the face,\n",
    "# its suitable for smaller input size.\n",
    "\n",
    "# other pretrained models we can use r ResNet or VGG: but they r heavier and require lrger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73af551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70680c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 4s 0us/step\n",
      "9420800/9406464 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "\n",
    "# pretrained weights of the Mobilenetv2 are beign loaded,y do we need pretrained weights?? because they r the stored features,\n",
    "# spatial relationships,curves,edges, textures of 1.2 million images where face is also a part of.\n",
    "\n",
    "# we r defining inputsize of each image as 224,224,3\n",
    "\n",
    "# include_top=False means we r removing the ann base used to classify imageslike cat ,dog, etc but we r keeping the \n",
    "\n",
    "# cnn base where the features r stored. y is it cos ann is the part where its unique for evry model?\n",
    "# its always better to create the ann part of every model to improve accuracy uncless our model also has 1000 classes like \n",
    "# mobilevnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecd6ca",
   "metadata": {},
   "source": [
    "# 6. now we r going to create our own custom head or the ann part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff02ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we r going to build our own custom ann head part:\n",
    "\n",
    "\n",
    "head_model = base_model.output # this loads the cnn part \n",
    "head_model = AveragePooling2D(pool_size=(7, 7))(head_model)\n",
    "head_model = Flatten(name=\"flatten\")(head_model)\n",
    "head_model = Dense(128, activation=\"relu\")(head_model)\n",
    "head_model = Dropout(0.5)(head_model)\n",
    "head_model = Dense(2, activation=\"softmax\")(head_model)\n",
    "\n",
    "# here after each line of code we r writing (head_model) this is linking in keras where we link the output from the previeous\n",
    "# layer to the output from the next layer\n",
    "# head_model = base_model.output output from this is linked to AveragePooling2D(pool_size=(7, 7)) and saved in to head model\n",
    "# and this then linked to Flatten(name=\"flatten\"), till the final output layer and till we get the final head model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69565afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30434ec6",
   "metadata": {},
   "source": [
    "# 7. now we r going to join the pretrained model and the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b08c7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "# here inputs=base_model.input here we passing the input layer of the base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a00376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False #  this will prevent the traing of the base layrs of MobileNetV2 during traing of the model and \n",
    "    # will train only the head part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5646869",
   "metadata": {},
   "source": [
    "# 8. we r going to do model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ee9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "INIT_LR = 1e-4       # Learning rate\n",
    "         \n",
    "opt = Adam(learning_rate=INIT_LR) # hyperparameters of adam y do u give? Smaller LR = slower learning (more stable)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",   # since 2 classes\n",
    "    optimizer=opt,\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4914a54",
   "metadata": {},
   "source": [
    "# 9. we rgoing to do model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb79a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "84/84 [==============================] - 269s 3s/step - loss: 0.4460 - accuracy: 0.8539 - val_loss: 0.1962 - val_accuracy: 0.9686\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 342s 4s/step - loss: 0.1945 - accuracy: 0.9496 - val_loss: 0.1092 - val_accuracy: 0.9704\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 230s 3s/step - loss: 0.1297 - accuracy: 0.9623 - val_loss: 0.0812 - val_accuracy: 0.9738\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 189s 2s/step - loss: 0.0952 - accuracy: 0.9757 - val_loss: 0.0674 - val_accuracy: 0.9765\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 230s 3s/step - loss: 0.0815 - accuracy: 0.9776 - val_loss: 0.0580 - val_accuracy: 0.9808\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 195s 2s/step - loss: 0.0660 - accuracy: 0.9862 - val_loss: 0.0507 - val_accuracy: 0.9817\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 221s 3s/step - loss: 0.0623 - accuracy: 0.9836 - val_loss: 0.0460 - val_accuracy: 0.9808\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 218s 3s/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 0.0428 - val_accuracy: 0.9826\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 227s 3s/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.0404 - val_accuracy: 0.9861\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 191s 2s/step - loss: 0.0466 - accuracy: 0.9873 - val_loss: 0.0380 - val_accuracy: 0.9869\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 214s 3s/step - loss: 0.0419 - accuracy: 0.9895 - val_loss: 0.0358 - val_accuracy: 0.9861\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 198s 2s/step - loss: 0.0354 - accuracy: 0.9922 - val_loss: 0.0348 - val_accuracy: 0.9869\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 180s 2s/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.0336 - val_accuracy: 0.9861\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 252s 3s/step - loss: 0.0320 - accuracy: 0.9929 - val_loss: 0.0313 - val_accuracy: 0.9878\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 194s 2s/step - loss: 0.0283 - accuracy: 0.9940 - val_loss: 0.0305 - val_accuracy: 0.9887\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 192s 2s/step - loss: 0.0267 - accuracy: 0.9944 - val_loss: 0.0297 - val_accuracy: 0.9869\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 209s 2s/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.0291 - val_accuracy: 0.9878\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 184s 2s/step - loss: 0.0237 - accuracy: 0.9944 - val_loss: 0.0282 - val_accuracy: 0.9878\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - 224s 3s/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.0280 - val_accuracy: 0.9887\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 237s 3s/step - loss: 0.0210 - accuracy: 0.9951 - val_loss: 0.0268 - val_accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162d217",
   "metadata": {},
   "source": [
    "# 10 now we predict y_test and convert them to readable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2af028e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e36a9",
   "metadata": {},
   "source": [
    "# 11. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38c66b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1102a259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Confusion Matrix')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 2) Plot the heatmap with center-aligned annotations and contrasting text color\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",cmap=\"Blues\",annot_kws={ \"ha\": \"center\",  \"va\": \"center\",  \"color\": \"white\",\"fontsize\": 12}\n",
    "            ,linewidths=2, linecolor='white',cbar=False)\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa59836",
   "metadata": {},
   "source": [
    "# 12. classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33f2edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       576\n",
      "           1       0.99      0.99      0.99       571\n",
      "\n",
      "    accuracy                           0.99      1147\n",
      "   macro avg       0.99      0.99      0.99      1147\n",
      "weighted avg       0.99      0.99      0.99      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858646ea",
   "metadata": {},
   "source": [
    "# 13. save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb83d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimmy\\anaconda3\\envs\\aiml\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask_detector_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ddaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
